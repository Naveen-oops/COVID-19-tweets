{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# python imports\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# twitter\n",
    "# pip install twarc\n",
    "from twarc import Twarc\n",
    "import carmen\n",
    "\n",
    "resolver = carmen.get_resolver()\n",
    "resolver.load_locations()\n",
    "\n",
    "\n",
    "# Saving models\n",
    "import pickle\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the tweets and updating their locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getLocation(t):\n",
    "    fields = ['entities', 'user', 'place', 'coordinates']\n",
    "    for field in fields:\n",
    "        if(pd.isna(t[field]) or t[field].isnumeric() or (t[field] == 'True' or t[field] == 'False')):\n",
    "            t[field] = dict()\n",
    "        else:\n",
    "            t[field] = eval(t[field])\n",
    "    \n",
    "    location = resolver.resolve_tweet(t)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading... 05-01-2020_full.csv\n",
      "05-01-2020_full.csv ..done\n",
      "\n",
      "reading... 05-02-2020_full.csv\n",
      "05-02-2020_full.csv ..done\n",
      "\n",
      "reading... 05-03-2020_full.csv\n",
      "05-03-2020_full.csv ..done\n",
      "\n",
      "reading... 05-04-2020_full.csv\n",
      "05-04-2020_full.csv ..done\n",
      "\n",
      "reading... 05-05-2020_full.csv\n",
      "05-05-2020_full.csv ..done\n",
      "\n",
      "reading... 05-06-2020_full.csv\n",
      "05-06-2020_full.csv ..done\n",
      "\n",
      "reading... 05-07-2020_full.csv\n"
     ]
    }
   ],
   "source": [
    "main_dir = './'\n",
    "full_dir = main_dir + 'data_full/'\n",
    "id_dir = main_dir + 'data_ids/'\n",
    "\n",
    "df_list = []\n",
    "\n",
    "for file in sorted(os.listdir(full_dir)):\n",
    "    print('reading... ' + file)\n",
    "    \n",
    "    df_full = pd.read_csv(full_dir + file).drop_duplicates()\n",
    "#     df_full = df_full.dropna(subset = ['id'])\n",
    "    \n",
    "#     df_full.to_csv(full_dir + file, index = None)\n",
    "    \n",
    "    df_full['locations'] = df_full.apply(getLocation, axis = 1)\n",
    "    df_full[['id', 'locations']].to_csv(full_dir + file[:-9] + '_loc.csv', index = None)\n",
    "    df_list.append(df_full)\n",
    "    \n",
    "    print(file + ' ..done')\n",
    "    print()\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Location(country='Nigeria', state='Federal Capital Territory', county='Abuja Municipal Area Council', city='Abuja', known=True, id=4775)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_list[0].locations[1][1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal frequency of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set(title='Temporal tweet frequency worldwide', xlabel='Time', ylabel='Tweet frequency per hour')\n",
    "plt.hist(pd.to_datetime(df.created_at), bins = 24*10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking out the tweet texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = df['text']\n",
    "text_en.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr = text_en.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
    "text_en_lr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr_lc = text_en_lr.apply(lambda x: x.lower())\n",
    "text_en_lr_lc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr_lc_pr = text_en_lr_lc.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "text_en_lr_lc_pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19'])\n",
    "\n",
    "text_en_lr_lc_pr_sr = text_en_lr_lc_pr.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "text_en_lr_lc_pr_sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all the tweets into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word for line in text_en_lr_lc_pr_sr for word in line.split()]\n",
    "word_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "counts = Counter(word_list).most_common(50)\n",
    "counts_df = pd.DataFrame(counts)\n",
    "counts_df\n",
    "counts_df.columns = ['word', 'frequency']\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "ax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df)\n",
    "plt.savefig('wordcount_bar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(word_list))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.savefig('wordcloud.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the polarity scores for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = text_en_lr_lc_pr_sr.apply(lambda x: sid.polarity_scores(x))\n",
    "sent_scores_df = pd.DataFrame(list(sentiment_scores))\n",
    "sent_scores_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the scores based on the compount polarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores_df['val'] = sent_scores_df['compound'].apply(lambda x: 'neutral' if x == 0 else ('positive' if x > 0 else 'negative'))\n",
    "sent_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the sentiment score counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_counts = pd.DataFrame.from_dict(Counter(sent_scores_df['val']), orient = 'index').reset_index()\n",
    "sent_counts.columns = ['sentiment', 'count']\n",
    "\n",
    "sns.barplot(y=\"count\", x='sentiment', data=sent_counts)\n",
    "plt.savefig('sentiment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal plot of the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_time_df = pd.DataFrame()\n",
    "sentiments_time_df['time'] = df['created_at']\n",
    "sentiments_time_df['polarity'] = sent_scores_df['compound']\n",
    "sentiments_time_df.index = pd.to_datetime(sentiments_time_df['time'])\n",
    "\n",
    "\n",
    "ot = sentiments_time_df.sample(frac=.001)\n",
    "ot['time'] = pd.to_datetime(ot['time'])\n",
    "ot.index = pd.to_datetime(ot['time'])\n",
    "ot.sort_index(inplace=True)\n",
    "ot['expanding'] = ot['polarity'].expanding().mean()\n",
    "ot['rolling'] = ot['polarity'].rolling('1h').mean()\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(ot['time'],ot['polarity'], label='Tweet Sentiment', s = 10, color = 'y')\n",
    "ax.plot(ot['time'],ot['rolling'], color ='r', label='Rolling Mean', linewidth = 5)\n",
    "ax.plot(ot['time'],ot['expanding'], color='b', label='Expanding Mean', linewidth = 5)\n",
    "ax.set_xlim([dt.date(2020,5,1),dt.date(2020,5,9)])\n",
    "ax.set(title='Tweet Sentiments over Time', xlabel='Date', ylabel='Sentiment polarity')\n",
    "ax.legend(loc='best')\n",
    "fig.tight_layout()\n",
    "plt.savefig('temporal_sentiments.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set(title='Tweet Sentiments distribution', xlabel='polarity', ylabel='frequency')\n",
    "sns.distplot(sentiments_time_df['polarity'], bins=30, ax=ax)\n",
    "# plt.show()\n",
    "plt.savefig('sentiment_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud of polar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_tweets_df = pd.DataFrame()\n",
    "polar_tweets_df['tweet'] = text_en_lr_lc_pr_sr\n",
    "polar_tweets_df['polarity'] = sent_scores_df['val']\n",
    "\n",
    "positive = polar_tweets_df[polar_tweets_df['polarity'] == 'positive']['tweet']\n",
    "negative = polar_tweets_df[polar_tweets_df['polarity'] == 'negative']['tweet']\n",
    "neutral = polar_tweets_df[polar_tweets_df['polarity'] == 'neutral']['tweet']\n",
    "\n",
    "positive_list = [word for line in positive for word in line.split()]\n",
    "negative_list = [word for line in negative for word in line.split()]\n",
    "neutral_list = [word for line in neutral for word in line.split()]\n",
    "\n",
    "positive_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(positive_list))\n",
    "\n",
    "negative_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(negative_list))\n",
    "\n",
    "neutral_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral_list))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
    "# fig.suptitle('Clouds of polar words', fontsize = 30)\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(positive_cloud)\n",
    "axs[0, 0].set_title('Words from positive tweets', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "# axs[0, 0].tight_layout(pad = 1)\n",
    "\n",
    "axs[0, 1].imshow(negative_cloud)\n",
    "axs[0, 1].set_title('Words from negative tweets', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "# axs[0, 1].tight_layout(pad = 1)\n",
    "\n",
    "axs[1, 0].imshow(neutral_cloud)\n",
    "axs[1, 0].set_title('Words from neutral tweets', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "# axs[1, 0].tight_layout(pad = 1)\n",
    "\n",
    "axs[1, 1].imshow(wordcloud)\n",
    "axs[1, 1].set_title('Words from all tweets', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "# axs[1, 0].tight_layout(pad = 1)\n",
    "plt.savefig('joint_cloud.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
