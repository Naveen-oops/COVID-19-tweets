{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import random\n",
    "import string\n",
    "import warnings\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.corpus import words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.sentiment.util import *\n",
    "\n",
    "\n",
    "# sklearn imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import metrics\n",
    "\n",
    "\n",
    "# python imports\n",
    "import re\n",
    "import json\n",
    "import os\n",
    "from collections import Counter\n",
    "import datetime as dt\n",
    "\n",
    "\n",
    "# Visualization\n",
    "from matplotlib import pyplot as plt\n",
    "from matplotlib import ticker\n",
    "import seaborn as sns\n",
    "from sklearn import feature_extraction, linear_model, model_selection, preprocessing\n",
    "from wordcloud import WordCloud\n",
    "from tqdm import tqdm_notebook\n",
    "\n",
    "# twitter\n",
    "# pip install twarc\n",
    "from twarc import Twarc\n",
    "import carmen\n",
    "\n",
    "resolver = carmen.get_resolver()\n",
    "resolver.load_locations()\n",
    "\n",
    "\n",
    "# Saving models\n",
    "import pickle\n",
    "\n",
    "import config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reading the tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of full df:  329567\n",
      "length of ids df:  174293\n",
      "\n",
      "duplicates in full df:  155948\n",
      "duplicates in ids df:  5\n",
      "\n",
      "null values in full df:  6\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  173613\n",
      "new length of ids df:  174288\n",
      "\n",
      "\n",
      "length of full df:  283106\n",
      "length of ids df:  113267\n",
      "\n",
      "duplicates in full df:  170656\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  3\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  112447\n",
      "new length of ids df:  113267\n",
      "\n",
      "\n",
      "length of full df:  250045\n",
      "length of ids df:  104108\n",
      "\n",
      "duplicates in full df:  146548\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  9\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  103488\n",
      "new length of ids df:  104108\n",
      "\n",
      "\n",
      "length of full df:  328001\n",
      "length of ids df:  162165\n",
      "\n",
      "duplicates in full df:  166674\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  9\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  161318\n",
      "new length of ids df:  162165\n",
      "\n",
      "\n",
      "length of full df:  401875\n",
      "length of ids df:  179617\n",
      "\n",
      "duplicates in full df:  223322\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  4\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  178549\n",
      "new length of ids df:  179617\n",
      "\n",
      "\n",
      "length of full df:  398346\n",
      "length of ids df:  191156\n",
      "\n",
      "duplicates in full df:  213137\n",
      "duplicates in ids df:  1\n",
      "\n",
      "null values in full df:  4\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  185205\n",
      "new length of ids df:  191155\n",
      "\n",
      "\n",
      "length of full df:  363677\n",
      "length of ids df:  189414\n",
      "\n",
      "duplicates in full df:  175358\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  12\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  188307\n",
      "new length of ids df:  189414\n",
      "\n",
      "\n",
      "length of full df:  291343\n",
      "length of ids df:  175165\n",
      "\n",
      "duplicates in full df:  125607\n",
      "duplicates in ids df:  1\n",
      "\n",
      "null values in full df:  7\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  165729\n",
      "new length of ids df:  175164\n",
      "\n",
      "\n",
      "length of full df:  220025\n",
      "length of ids df:  131585\n",
      "\n",
      "duplicates in full df:  96179\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  4\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  123842\n",
      "new length of ids df:  131585\n",
      "\n",
      "\n",
      "length of full df:  355897\n",
      "length of ids df:  142286\n",
      "\n",
      "duplicates in full df:  222821\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  3\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  133073\n",
      "new length of ids df:  142286\n",
      "\n",
      "\n",
      "length of full df:  332114\n",
      "length of ids df:  184949\n",
      "\n",
      "duplicates in full df:  157288\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  7\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  174819\n",
      "new length of ids df:  184949\n",
      "\n",
      "\n",
      "length of full df:  360840\n",
      "length of ids df:  181657\n",
      "\n",
      "duplicates in full df:  188658\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  6\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  172176\n",
      "new length of ids df:  181657\n",
      "\n",
      "\n",
      "length of full df:  291966\n",
      "length of ids df:  159157\n",
      "\n",
      "duplicates in full df:  140712\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  4\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  151250\n",
      "new length of ids df:  159157\n",
      "\n",
      "\n",
      "length of full df:  328968\n",
      "length of ids df:  170861\n",
      "\n",
      "duplicates in full df:  165868\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  3\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  163097\n",
      "new length of ids df:  170861\n",
      "\n",
      "\n",
      "length of full df:  290852\n",
      "length of ids df:  163497\n",
      "\n",
      "duplicates in full df:  135016\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  7\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  155829\n",
      "new length of ids df:  163497\n",
      "\n",
      "\n",
      "length of full df:  247812\n",
      "length of ids df:  114830\n",
      "\n",
      "duplicates in full df:  138128\n",
      "duplicates in ids df:  0\n",
      "\n",
      "null values in full df:  4\n",
      "null values in ids df:  0\n",
      "\n",
      "new length of full df:  109680\n",
      "new length of ids df:  114830\n",
      "\n",
      "\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] File b'./COVID-19-tweets/data_ids/dataset-meta_ids.csv' does not exist: b'./COVID-19-tweets/data_ids/dataset-meta_ids.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-148-166e708674fa>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mdf_full\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m     \u001b[0mdf_ids\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid_dir\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m9\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m'_ids.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of full df: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_full\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"length of ids df: \"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36mparser_f\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, dialect, tupleize_cols, error_bad_lines, warn_bad_lines, delim_whitespace, low_memory, memory_map, float_precision)\u001b[0m\n\u001b[1;32m    700\u001b[0m                     skip_blank_lines=skip_blank_lines)\n\u001b[1;32m    701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 702\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    703\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    704\u001b[0m     \u001b[0mparser_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    427\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 429\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    430\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    431\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'has_index_names'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, engine)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mengine\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'c'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1121\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'c'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1122\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCParserWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1123\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1124\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mengine\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'python'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/pandas/io/parsers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, src, **kwds)\u001b[0m\n\u001b[1;32m   1851\u001b[0m         \u001b[0mkwds\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'usecols'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0musecols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1853\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mparsers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTextReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msrc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1854\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munnamed_cols\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1855\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/_libs/parsers.pyx\u001b[0m in \u001b[0;36mpandas._libs.parsers.TextReader._setup_parser_source\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] File b'./COVID-19-tweets/data_ids/dataset-meta_ids.csv' does not exist: b'./COVID-19-tweets/data_ids/dataset-meta_ids.csv'"
     ]
    }
   ],
   "source": [
    "main_dir = './'\n",
    "full_dir = main_dir + 'data_full/'\n",
    "id_dir = main_dir + 'data_ids/'\n",
    "\n",
    "for file in sorted(os.listdir(full_dir)):\n",
    "    \n",
    "    df_full = pd.read_csv(full_dir + file)\n",
    "    df_ids = pd.read_csv(id_dir + file[:-9] + '_ids.csv')\n",
    "    print(\"length of full df: \", len(df_full))\n",
    "    print(\"length of ids df: \", len(df_ids))\n",
    "    print()\n",
    "    print(\"duplicates in full df: \", len(df_full[df_full.duplicated()]))\n",
    "    print(\"duplicates in ids df: \", len(df_ids[df_ids.duplicated()]))\n",
    "    print()\n",
    "    df_full = df_full.drop_duplicates()\n",
    "    df_ids = df_ids.drop_duplicates()\n",
    "    \n",
    "#     print(\"new length of full df: \", len(df_full))\n",
    "#     print(\"new length of ids df: \", len(df_ids))\n",
    "    \n",
    "    print(\"null values in full df: \", df_full.id.isna().sum())\n",
    "    print(\"null values in ids df: \", df_ids.id.isna().sum())\n",
    "    df_full = df_full.dropna(subset = ['id'])\n",
    "    df_ids = df_ids.dropna(subset = ['id'])\n",
    "    print()\n",
    "    print(\"new length of full df: \", len(df_full))\n",
    "    print(\"new length of ids df: \", len(df_ids))\n",
    "    \n",
    "    df_full.to_csv(full_dir + file, index = None)\n",
    "    df_ids.to_csv(id_dir + file[:-9] + '_ids.csv', index = None)\n",
    "    \n",
    "    print()\n",
    "    print()\n",
    "    \n",
    "# df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "163497\n",
      "163497\n",
      "id    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "data_dir = './COVID-19-tweets/data_ids/'\n",
    "\n",
    "t_df = pd.read_csv(data_dir + '05-15-2020_ids.csv', lineterminator = '\\n')\n",
    "print(len(t_df))\n",
    "print(t_df.id.drop_duplicates().count())\n",
    "print(t_df.isna().sum())\n",
    "# t_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_location(t):\n",
    "    fields = ['entities', 'user', 'place', 'coordinates']\n",
    "    for field in fields:\n",
    "        if(pd.isna(t[field])):\n",
    "            t[field] = dict()\n",
    "        else:\n",
    "            t[field] = eval(t[field])\n",
    "    \n",
    "    location = resolver.resolve_tweet(t)\n",
    "    return location"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['location'] = df.apply(get_location, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "created_at                   185208\n",
       "id                           185205\n",
       "id_str                       185205\n",
       "full_text                    185205\n",
       "truncated                    185201\n",
       "display_text_range           185197\n",
       "entities                     185197\n",
       "source                       185195\n",
       "in_reply_to_status_id         22884\n",
       "in_reply_to_status_id_str     22884\n",
       "in_reply_to_user_id           28600\n",
       "in_reply_to_user_id_str       28596\n",
       "in_reply_to_screen_name       28596\n",
       "user                         185197\n",
       "geo                             812\n",
       "coordinates                     816\n",
       "place                          7433\n",
       "contributors                      4\n",
       "is_quote_status              185201\n",
       "retweet_count                185201\n",
       "favorite_count               185201\n",
       "favorited                    185201\n",
       "retweeted                    185197\n",
       "possibly_sensitive           139761\n",
       "lang                         185197\n",
       "dtype: int64"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.drop_duplicates().count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal frequency of tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(25,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set(title='Temporal tweet frequency worldwide', xlabel='Time', ylabel='Tweet frequency per hour')\n",
    "plt.hist(pd.to_datetime(df.created_at), bins = 24*10)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Picking out the tweet texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en = df['text']\n",
    "text_en.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing URLs from tweets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr = text_en.apply(lambda x: re.sub(r\"https\\S+\", \"\", str(x)))\n",
    "text_en_lr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Converting all tweets to lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr_lc = text_en_lr.apply(lambda x: x.lower())\n",
    "text_en_lr_lc.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing punctuations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "text_en_lr_lc_pr = text_en_lr_lc.apply(lambda x: x.translate(str.maketrans('', '', string.punctuation)))\n",
    "text_en_lr_lc_pr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))\n",
    "stop_words.update(['#coronavirus', '#coronavirusoutbreak', '#coronavirusPandemic', '#covid19', '#covid_19', '#epitwitter', '#ihavecorona', 'amp', 'coronavirus', 'covid19'])\n",
    "\n",
    "text_en_lr_lc_pr_sr = text_en_lr_lc_pr.apply(lambda x: ' '.join([word for word in x.split() if word not in stop_words]))\n",
    "text_en_lr_lc_pr_sr.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concatenating all the tweets into a list of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_list = [word for line in text_en_lr_lc_pr_sr for word in line.split()]\n",
    "word_list[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating the Term Frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set(style=\"darkgrid\")\n",
    "counts = Counter(word_list).most_common(50)\n",
    "counts_df = pd.DataFrame(counts)\n",
    "counts_df\n",
    "counts_df.columns = ['word', 'frequency']\n",
    "\n",
    "fig, ax = plt.subplots(figsize = (12, 12))\n",
    "ax = sns.barplot(y=\"word\", x='frequency', ax = ax, data=counts_df)\n",
    "plt.savefig('wordcount_bar.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wordcloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(word_list))\n",
    "\n",
    "\n",
    "plt.figure(figsize = (12, 10), facecolor = None)\n",
    "plt.imshow(wordcloud)\n",
    "plt.axis(\"off\")\n",
    "plt.tight_layout(pad = 0)\n",
    "\n",
    "plt.savefig('wordcloud.png')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting the polarity scores for each tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sid = SentimentIntensityAnalyzer()\n",
    "sentiment_scores = text_en_lr_lc_pr_sr.apply(lambda x: sid.polarity_scores(x))\n",
    "sent_scores_df = pd.DataFrame(list(sentiment_scores))\n",
    "sent_scores_df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classifying the scores based on the compount polarity value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_scores_df['val'] = sent_scores_df['compound'].apply(lambda x: 'neutral' if x == 0 else ('positive' if x > 0 else 'negative'))\n",
    "sent_scores_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plotting the sentiment score counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_counts = pd.DataFrame.from_dict(Counter(sent_scores_df['val']), orient = 'index').reset_index()\n",
    "sent_counts.columns = ['sentiment', 'count']\n",
    "\n",
    "sns.barplot(y=\"count\", x='sentiment', data=sent_counts)\n",
    "plt.savefig('sentiment.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Temporal plot of the sentiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments_time_df = pd.DataFrame()\n",
    "sentiments_time_df['time'] = df['created_at']\n",
    "sentiments_time_df['polarity'] = sent_scores_df['compound']\n",
    "sentiments_time_df.index = pd.to_datetime(sentiments_time_df['time'])\n",
    "\n",
    "\n",
    "ot = sentiments_time_df.sample(frac=.001)\n",
    "ot['time'] = pd.to_datetime(ot['time'])\n",
    "ot.index = pd.to_datetime(ot['time'])\n",
    "ot.sort_index(inplace=True)\n",
    "ot['expanding'] = ot['polarity'].expanding().mean()\n",
    "ot['rolling'] = ot['polarity'].rolling('1h').mean()\n",
    "\n",
    "fig = plt.figure(figsize=(20,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.scatter(ot['time'],ot['polarity'], label='Tweet Sentiment', s = 10, color = 'y')\n",
    "ax.plot(ot['time'],ot['rolling'], color ='r', label='Rolling Mean', linewidth = 5)\n",
    "ax.plot(ot['time'],ot['expanding'], color='b', label='Expanding Mean', linewidth = 5)\n",
    "ax.set_xlim([dt.date(2020,5,1),dt.date(2020,5,9)])\n",
    "ax.set(title='Tweet Sentiments over Time', xlabel='Date', ylabel='Sentiment polarity')\n",
    "ax.legend(loc='best')\n",
    "fig.tight_layout()\n",
    "plt.savefig('temporal_sentiments.png')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment scores distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure(figsize=(10,5))\n",
    "ax = fig.add_subplot(111)\n",
    "ax.set(title='Tweet Sentiments distribution', xlabel='polarity', ylabel='frequency')\n",
    "sns.distplot(sentiments_time_df['polarity'], bins=30, ax=ax)\n",
    "# plt.show()\n",
    "plt.savefig('sentiment_distribution.png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Word cloud of polar words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "polar_tweets_df = pd.DataFrame()\n",
    "polar_tweets_df['tweet'] = text_en_lr_lc_pr_sr\n",
    "polar_tweets_df['polarity'] = sent_scores_df['val']\n",
    "\n",
    "positive = polar_tweets_df[polar_tweets_df['polarity'] == 'positive']['tweet']\n",
    "negative = polar_tweets_df[polar_tweets_df['polarity'] == 'negative']['tweet']\n",
    "neutral = polar_tweets_df[polar_tweets_df['polarity'] == 'neutral']['tweet']\n",
    "\n",
    "positive_list = [word for line in positive for word in line.split()]\n",
    "negative_list = [word for line in negative for word in line.split()]\n",
    "neutral_list = [word for line in neutral for word in line.split()]\n",
    "\n",
    "positive_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(positive_list))\n",
    "\n",
    "negative_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(negative_list))\n",
    "\n",
    "neutral_cloud = WordCloud(\n",
    "    background_color='black',\n",
    "    max_words=50,\n",
    "    max_font_size=40, \n",
    "    scale=5,\n",
    "    random_state=1,\n",
    "    collocations=False,\n",
    "    normalize_plurals=False\n",
    ").generate(' '.join(neutral_list))\n",
    "\n",
    "\n",
    "fig, axs = plt.subplots(2, 2, figsize = (20, 12))\n",
    "# fig.suptitle('Clouds of polar words', fontsize = 30)\n",
    "fig.tight_layout(pad = 0)\n",
    "\n",
    "axs[0, 0].imshow(positive_cloud)\n",
    "axs[0, 0].set_title('Words from positive tweets', fontsize = 20)\n",
    "axs[0, 0].axis('off')\n",
    "# axs[0, 0].tight_layout(pad = 1)\n",
    "\n",
    "axs[0, 1].imshow(negative_cloud)\n",
    "axs[0, 1].set_title('Words from negative tweets', fontsize = 20)\n",
    "axs[0, 1].axis('off')\n",
    "# axs[0, 1].tight_layout(pad = 1)\n",
    "\n",
    "axs[1, 0].imshow(neutral_cloud)\n",
    "axs[1, 0].set_title('Words from neutral tweets', fontsize = 20)\n",
    "axs[1, 0].axis('off')\n",
    "# axs[1, 0].tight_layout(pad = 1)\n",
    "\n",
    "axs[1, 1].imshow(wordcloud)\n",
    "axs[1, 1].set_title('Words from all tweets', fontsize = 20)\n",
    "axs[1, 1].axis('off')\n",
    "# axs[1, 0].tight_layout(pad = 1)\n",
    "plt.savefig('joint_cloud.png')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
